{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b675895b-6abb-4333-b41a-1b5be3e1ba08",
   "metadata": {},
   "source": [
    "## Importing necessary libraries \n",
    "- pandas : Used for data manipulation and analysis using DataFrames.\n",
    "- re: Provides regular expression operations for pattern matching in text.\n",
    "- TextBlob: A library for processing textual data, mainly used for sentiment analysis and NLP.\n",
    "- symspellpy: A fast spelling correction library using Symmetric Delete algorithm.\n",
    "- SpellChecker: Provides simple spell checking and correction for English words.\n",
    "- nltk: Natural Language Toolkit, used for working with human language data (text).\n",
    "- nltk.corpus import words: A list of valid English words from the NLTK corpus.\n",
    "- nltk.corpus import stopwords: Provides a list of common stopwords (e.g., \"the\", \"is\") for text cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e56810-95bf-4433-9049-71494e6bb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.corpus import words as nltk_words\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ceae30-a375-4ba7-813d-cdf5738c236e",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92d33e2-e6e4-4ece-a8fd-eae1d971ce90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct absolute path\n",
    "df = pd.read_csv(\"/Users/prarthana/Downloads/Reviews.csv\")\n",
    "\n",
    "# Preview the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ff0246-9747-4b31-bd32-6576ee7aa2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset (rows, columns): (568454, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of dataset (rows, columns):\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e225ecf-f0f7-43c3-86ba-e685690005bb",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Step 1: Checking data types of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "731b2f01-c4dc-459a-a118-35d0f370f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                         int64\n",
      "ProductId                 object\n",
      "UserId                    object\n",
      "ProfileName               object\n",
      "HelpfulnessNumerator       int64\n",
      "HelpfulnessDenominator     int64\n",
      "Score                      int64\n",
      "Time                       int64\n",
      "Summary                   object\n",
      "Text                      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286d90a-bc91-4422-b610-659967fe6b2e",
   "metadata": {},
   "source": [
    "### Step 2: Changing data types where required\n",
    "Since ProductId and UserId are unique identifiers for products and users, converting them to categorical and Time to datetime \n",
    "\n",
    "Note: These columns are not going to be used as only summary and text columns will be required for this analysis, but following the cleaning steps for ease of use for further analysis on the same data, done on a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9379fb-380c-4f96-931d-e1cb3698246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'ProductId' and 'UserId' to categorical data type\n",
    "df['ProductId'] = df['ProductId'].astype('category')\n",
    "df['UserId'] = df['UserId'].astype('category')\n",
    "\n",
    "# Convert 'Time' from Unix timestamp to datetime\n",
    "df['Time'] = pd.to_datetime(df['Time'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7b06306-01a1-468a-aa9d-1dc2ac4c06fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-04-27</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-09-07</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-06-13</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-10-21</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "0                     1                       1      5 2011-04-27   \n",
       "1                     0                       0      1 2012-09-07   \n",
       "2                     1                       1      4 2008-08-18   \n",
       "3                     3                       3      2 2011-06-13   \n",
       "4                     0                       0      5 2012-10-21   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8a80f-ce64-4094-857d-c49d9e4b33bb",
   "metadata": {},
   "source": [
    "### Step 3: Checking for null values\n",
    "\n",
    "Here since the null values are a fairly insignificant number and present in profile name and summary, we move forward without treating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d1f4e0-0e63-40ba-ae61-d464b055d00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                         0\n",
      "ProductId                  0\n",
      "UserId                     0\n",
      "ProfileName               26\n",
      "HelpfulnessNumerator       0\n",
      "HelpfulnessDenominator     0\n",
      "Score                      0\n",
      "Time                       0\n",
      "Summary                   27\n",
      "Text                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb0df964-cdbb-4bb5-b6de-1284c25b5dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with at least one null value: 53\n"
     ]
    }
   ],
   "source": [
    "num_null_rows = df.isnull().any(axis=1).sum()\n",
    "print(\"Number of rows with at least one null value:\", num_null_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba4f63-189b-4f90-9bdb-5e8df84e4aa7",
   "metadata": {},
   "source": [
    "### Step 4: Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66e0711f-c92a-4c21-8bfb-3715fbe4f2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                        568454\n",
       "ProductId                  74258\n",
       "UserId                    256059\n",
       "ProfileName               218415\n",
       "HelpfulnessNumerator         231\n",
       "HelpfulnessDenominator       234\n",
       "Score                          5\n",
       "Time                        3168\n",
       "Summary                   295742\n",
       "Text                      393579\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_counts = df.nunique()\n",
    "\n",
    "# Print the result\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d696a3-714e-475d-a933-b7da460bba47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10616</th>\n",
       "      <td>10617</td>\n",
       "      <td>B002DHN956</td>\n",
       "      <td>A1LSYR30XW7CFT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2010-12-07</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>This is 72 for the price of like 24 at the gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25509</th>\n",
       "      <td>25510</td>\n",
       "      <td>B000LKZB4Y</td>\n",
       "      <td>A36BVYD0NT7Z0F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-08-29</td>\n",
       "      <td>These are the best mints and no aspartame or BHT</td>\n",
       "      <td>I was so shocked to find out that almost all g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33958</th>\n",
       "      <td>33959</td>\n",
       "      <td>B00412W76S</td>\n",
       "      <td>A3TJPSWY2HE4BS</td>\n",
       "      <td>S. Layton \"homeschool blogger\"</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2007-03-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I only used two maybe three tea bags and got p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38874</th>\n",
       "      <td>38875</td>\n",
       "      <td>B000AYDGZ2</td>\n",
       "      <td>A36BVYD0NT7Z0F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-06</td>\n",
       "      <td>doesn't anyone care that they are putting BHT ...</td>\n",
       "      <td>I called Kellogg's to see why Special K red be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40548</th>\n",
       "      <td>40549</td>\n",
       "      <td>B00020HHRW</td>\n",
       "      <td>A3TJPSWY2HE4BS</td>\n",
       "      <td>S. Layton \"homeschool blogger\"</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2007-03-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I only used two maybe three tea bags and got p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47923</th>\n",
       "      <td>47924</td>\n",
       "      <td>B004SRH2B6</td>\n",
       "      <td>A2DEHJJIEAPPBF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-06-06</td>\n",
       "      <td>Great taste</td>\n",
       "      <td>Enjoy drinking this brand.  Tastes as good as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49800</th>\n",
       "      <td>49801</td>\n",
       "      <td>B000CRHQN0</td>\n",
       "      <td>A2LYFY32LXQDON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-08-24</td>\n",
       "      <td>They were melted and the chocolate had turned ...</td>\n",
       "      <td>We love these bars but i won't order them ship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67077</th>\n",
       "      <td>67078</td>\n",
       "      <td>B0006348H2</td>\n",
       "      <td>A2P0P67Y55SNOX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-08-30</td>\n",
       "      <td>Wheatgrass</td>\n",
       "      <td>Kitty seems to like this sprinkled on her food...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101106</th>\n",
       "      <td>101107</td>\n",
       "      <td>B0014B0HWK</td>\n",
       "      <td>A3TJPSWY2HE4BS</td>\n",
       "      <td>S. Layton \"homeschool blogger\"</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2007-03-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I only used two maybe three tea bags and got p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102979</th>\n",
       "      <td>102980</td>\n",
       "      <td>B000FVDWU4</td>\n",
       "      <td>A3TJPSWY2HE4BS</td>\n",
       "      <td>S. Layton \"homeschool blogger\"</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2007-03-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I only used two maybe three tea bags and got p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                     ProfileName  \\\n",
       "10616    10617  B002DHN956  A1LSYR30XW7CFT                             NaN   \n",
       "25509    25510  B000LKZB4Y  A36BVYD0NT7Z0F                             NaN   \n",
       "33958    33959  B00412W76S  A3TJPSWY2HE4BS  S. Layton \"homeschool blogger\"   \n",
       "38874    38875  B000AYDGZ2  A36BVYD0NT7Z0F                             NaN   \n",
       "40548    40549  B00020HHRW  A3TJPSWY2HE4BS  S. Layton \"homeschool blogger\"   \n",
       "47923    47924  B004SRH2B6  A2DEHJJIEAPPBF                             NaN   \n",
       "49800    49801  B000CRHQN0  A2LYFY32LXQDON                             NaN   \n",
       "67077    67078  B0006348H2  A2P0P67Y55SNOX                             NaN   \n",
       "101106  101107  B0014B0HWK  A3TJPSWY2HE4BS  S. Layton \"homeschool blogger\"   \n",
       "102979  102980  B000FVDWU4  A3TJPSWY2HE4BS  S. Layton \"homeschool blogger\"   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "10616                      1                       2      5 2010-12-07   \n",
       "25509                      0                       0      5 2011-08-29   \n",
       "33958                      1                      24      2 2007-03-08   \n",
       "38874                      2                       3      1 2010-07-06   \n",
       "40548                      1                      24      2 2007-03-08   \n",
       "47923                      0                       0      5 2012-06-06   \n",
       "49800                      0                       0      2 2010-08-24   \n",
       "67077                      1                       1      5 2011-08-30   \n",
       "101106                     1                      24      2 2007-03-08   \n",
       "102979                     1                      24      2 2007-03-08   \n",
       "\n",
       "                                                  Summary  \\\n",
       "10616                                             Awesome   \n",
       "25509    These are the best mints and no aspartame or BHT   \n",
       "33958                                                 NaN   \n",
       "38874   doesn't anyone care that they are putting BHT ...   \n",
       "40548                                                 NaN   \n",
       "47923                                         Great taste   \n",
       "49800   They were melted and the chocolate had turned ...   \n",
       "67077                                          Wheatgrass   \n",
       "101106                                                NaN   \n",
       "102979                                                NaN   \n",
       "\n",
       "                                                     Text  \n",
       "10616   This is 72 for the price of like 24 at the gro...  \n",
       "25509   I was so shocked to find out that almost all g...  \n",
       "33958   I only used two maybe three tea bags and got p...  \n",
       "38874   I called Kellogg's to see why Special K red be...  \n",
       "40548   I only used two maybe three tea bags and got p...  \n",
       "47923   Enjoy drinking this brand.  Tastes as good as ...  \n",
       "49800   We love these bars but i won't order them ship...  \n",
       "67077   Kitty seems to like this sprinkled on her food...  \n",
       "101106  I only used two maybe three tea bags and got p...  \n",
       "102979  I only used two maybe three tea bags and got p...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().any(axis=1)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe9ba4c3-cb13-4c75-834c-b717efdfa5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = df.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", duplicate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c63666b-f3b8-4722-ad96-7eea56839e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows based on ['ProductId', 'UserId', 'ProfileName', 'Summary', 'Text']: 898\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to check for duplicates\n",
    "columns_to_check = ['ProductId', 'UserId', 'ProfileName', 'Summary', 'Text']\n",
    "\n",
    "# Count duplicates based on those columns\n",
    "dup_count = df.duplicated(subset=columns_to_check).sum()\n",
    "\n",
    "print(f\"Number of duplicate rows based on {columns_to_check}: {dup_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fc96a84-a363-470a-9604-2c3ac4c60310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in 'ProductId': 494196\n",
      "Duplicates in 'UserId': 312395\n",
      "Duplicates in 'ProfileName': 350038\n",
      "Duplicates in 'Summary': 272711\n",
      "Duplicates in 'Text': 174875\n"
     ]
    }
   ],
   "source": [
    "# List of specific columns to check for duplicates\n",
    "columns_to_check = ['ProductId', 'UserId', 'ProfileName', 'Summary', 'Text']\n",
    "\n",
    "# Loop through each column and count duplicates\n",
    "for col in columns_to_check:\n",
    "    dup_count = df.duplicated(subset=[col]).sum()\n",
    "    print(f\"Duplicates in '{col}': {dup_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68a202-3d56-4e05-817a-7e5cce880a4f",
   "metadata": {},
   "source": [
    "### Step 5: Treating Duplicates\n",
    "\n",
    "Checking for duplicates in text column, there is a significant number of repeated reviews. Since we are going to use this for sentiment analysis, we want to get rid of redundant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06416143-9f32-4713-ad8d-ae27eb70b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with duplicate 'Text' values: 232915\n"
     ]
    }
   ],
   "source": [
    "# Count rows where the 'Text' column is duplicated (entire 'Text' value is the same)\n",
    "dup_text_count = df['Text'].duplicated(keep=False).sum()\n",
    "\n",
    "print(f\"Number of rows with duplicate 'Text' values: {dup_text_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0392e07b-8a6d-4de7-99a0-d052783e55d4",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "Grouping the duplicated text to identify why they are not being flagged. Here many duplicated texts have different ProductId but same UserId, Summary, and Time. If there was further information on the product we could have identified if it was for different products but here we are assuming it is repeated reviews by the same user for the product being falsely classified as different ProductsIds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "059b9ed0-70b3-4ed6-81d0-3560a12bb20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257785</th>\n",
       "      <td>257786</td>\n",
       "      <td>B000KOWR8E</td>\n",
       "      <td>A142S4ZZF1FJ1X</td>\n",
       "      <td>Joseph E Brew</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-10-09</td>\n",
       "      <td>Better Sweetener!</td>\n",
       "      <td>\"4C Totally Light\" is one of the very few \"sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506745</th>\n",
       "      <td>506746</td>\n",
       "      <td>B000KOWR8Y</td>\n",
       "      <td>A142S4ZZF1FJ1X</td>\n",
       "      <td>Joseph E Brew</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-10-09</td>\n",
       "      <td>4C Totally Light</td>\n",
       "      <td>\"4C Totally Light\" is one of the very few \"sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107704</th>\n",
       "      <td>107705</td>\n",
       "      <td>B001F0RRTQ</td>\n",
       "      <td>A1R7E82MN0S8V3</td>\n",
       "      <td>DENNIS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-06-12</td>\n",
       "      <td>GREAT DOG TREAT</td>\n",
       "      <td>\"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418609</th>\n",
       "      <td>418610</td>\n",
       "      <td>B001F0RRU0</td>\n",
       "      <td>A1R7E82MN0S8V3</td>\n",
       "      <td>DENNIS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-06-12</td>\n",
       "      <td>GREAT DOG TREAT</td>\n",
       "      <td>\"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561246</th>\n",
       "      <td>561247</td>\n",
       "      <td>B001JU81ZG</td>\n",
       "      <td>A7FNPP1SMY97G</td>\n",
       "      <td>D. Hsu</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-11-08</td>\n",
       "      <td>Buy this if you have NO taste buds!</td>\n",
       "      <td>\"Blends smooth and creamy for a sweet tasting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330089</th>\n",
       "      <td>330090</td>\n",
       "      <td>B001OHX1ZY</td>\n",
       "      <td>A7FNPP1SMY97G</td>\n",
       "      <td>D. Hsu</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-11-08</td>\n",
       "      <td>Buy this if you have NO taste buds!</td>\n",
       "      <td>\"Blends smooth and creamy for a sweet tasting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233264</th>\n",
       "      <td>233265</td>\n",
       "      <td>B007TJGZ4A</td>\n",
       "      <td>A17950SQVNAVOD</td>\n",
       "      <td>Scott</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-18</td>\n",
       "      <td>Packaging quality problem</td>\n",
       "      <td>\"Both\" of Gloria Jean's \"Hazelnut\" and \"Vanill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473106</th>\n",
       "      <td>473107</td>\n",
       "      <td>B008FHUKE6</td>\n",
       "      <td>A17950SQVNAVOD</td>\n",
       "      <td>Scott</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-18</td>\n",
       "      <td>Packaging quality problem</td>\n",
       "      <td>\"Both\" of Gloria Jean's \"Hazelnut\" and \"Vanill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245224</th>\n",
       "      <td>245225</td>\n",
       "      <td>B0029XDZKI</td>\n",
       "      <td>A17950SQVNAVOD</td>\n",
       "      <td>Scott</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-18</td>\n",
       "      <td>Packaging quality problem</td>\n",
       "      <td>\"Both\" of Gloria Jean's \"Hazelnut\" and \"Vanill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425981</th>\n",
       "      <td>425982</td>\n",
       "      <td>B000TQEWM2</td>\n",
       "      <td>A17950SQVNAVOD</td>\n",
       "      <td>Scott</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-18</td>\n",
       "      <td>Packaging quality problem</td>\n",
       "      <td>\"Both\" of Gloria Jean's \"Hazelnut\" and \"Vanill...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId    ProfileName  \\\n",
       "257785  257786  B000KOWR8E  A142S4ZZF1FJ1X  Joseph E Brew   \n",
       "506745  506746  B000KOWR8Y  A142S4ZZF1FJ1X  Joseph E Brew   \n",
       "107704  107705  B001F0RRTQ  A1R7E82MN0S8V3         DENNIS   \n",
       "418609  418610  B001F0RRU0  A1R7E82MN0S8V3         DENNIS   \n",
       "561246  561247  B001JU81ZG   A7FNPP1SMY97G         D. Hsu   \n",
       "330089  330090  B001OHX1ZY   A7FNPP1SMY97G         D. Hsu   \n",
       "233264  233265  B007TJGZ4A  A17950SQVNAVOD          Scott   \n",
       "473106  473107  B008FHUKE6  A17950SQVNAVOD          Scott   \n",
       "245224  245225  B0029XDZKI  A17950SQVNAVOD          Scott   \n",
       "425981  425982  B000TQEWM2  A17950SQVNAVOD          Scott   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "257785                     2                       3      4 2010-10-09   \n",
       "506745                     0                       0      4 2010-10-09   \n",
       "107704                     0                       0      5 2012-06-12   \n",
       "418609                     0                       0      5 2012-06-12   \n",
       "561246                     4                       6      1 2011-11-08   \n",
       "330089                     4                       6      1 2011-11-08   \n",
       "233264                     2                       2      1 2011-08-18   \n",
       "473106                     2                       2      1 2011-08-18   \n",
       "245224                     2                       2      1 2011-08-18   \n",
       "425981                     2                       2      1 2011-08-18   \n",
       "\n",
       "                                    Summary  \\\n",
       "257785                    Better Sweetener!   \n",
       "506745                     4C Totally Light   \n",
       "107704                      GREAT DOG TREAT   \n",
       "418609                      GREAT DOG TREAT   \n",
       "561246  Buy this if you have NO taste buds!   \n",
       "330089  Buy this if you have NO taste buds!   \n",
       "233264            Packaging quality problem   \n",
       "473106            Packaging quality problem   \n",
       "245224            Packaging quality problem   \n",
       "425981            Packaging quality problem   \n",
       "\n",
       "                                                     Text  \n",
       "257785  \"4C Totally Light\" is one of the very few \"sug...  \n",
       "506745  \"4C Totally Light\" is one of the very few \"sug...  \n",
       "107704  \"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...  \n",
       "418609  \"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...  \n",
       "561246  \"Blends smooth and creamy for a sweet tasting ...  \n",
       "330089  \"Blends smooth and creamy for a sweet tasting ...  \n",
       "233264  \"Both\" of Gloria Jean's \"Hazelnut\" and \"Vanill...  \n",
       "473106  \"Both\" of Gloria Jean's \"Hazelnut\" and \"Vanill...  \n",
       "245224  \"Both\" of Gloria Jean's \"Hazelnut\" and \"Vanill...  \n",
       "425981  \"Both\" of Gloria Jean's \"Hazelnut\" and \"Vanill...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by 'Text' so that rows with the same 'Text' are grouped together\n",
    "dup_text_rows_sorted = df[df['Text'].duplicated(keep=False)].sort_values(by='Text')\n",
    "\n",
    "# Get the first 10 rows where the 'Text' column is duplicated and sorted by 'Text'\n",
    "dup_text_rows_head = dup_text_rows_sorted.head(10)\n",
    "\n",
    "# Print the first 10 rows with duplicated 'Text'\n",
    "dup_text_rows_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8857adc9-88c4-4828-8b2e-aba7375c5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in 'ProductId': 494196\n",
      "Duplicates in 'UserId': 312395\n",
      "Duplicates in 'ProfileName': 350038\n",
      "Duplicates in 'Summary': 272711\n",
      "Duplicates in 'Text': 174875\n"
     ]
    }
   ],
   "source": [
    "# List of specific columns to check for duplicates\n",
    "columns_to_check = ['ProductId', 'UserId', 'ProfileName', 'Summary', 'Text']\n",
    "\n",
    "# Loop through each column and count duplicates\n",
    "for col in columns_to_check:\n",
    "    dup_count = df.duplicated(subset=[col]).sum()\n",
    "    print(f\"Duplicates in '{col}': {dup_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6dc50a2-57f9-4fe6-966f-77afdd8d5859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257785</th>\n",
       "      <td>257786</td>\n",
       "      <td>B000KOWR8E</td>\n",
       "      <td>A142S4ZZF1FJ1X</td>\n",
       "      <td>Joseph E Brew</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-10-09</td>\n",
       "      <td>Better Sweetener!</td>\n",
       "      <td>\"4C Totally Light\" is one of the very few \"sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506745</th>\n",
       "      <td>506746</td>\n",
       "      <td>B000KOWR8Y</td>\n",
       "      <td>A142S4ZZF1FJ1X</td>\n",
       "      <td>Joseph E Brew</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-10-09</td>\n",
       "      <td>4C Totally Light</td>\n",
       "      <td>\"4C Totally Light\" is one of the very few \"sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107704</th>\n",
       "      <td>107705</td>\n",
       "      <td>B001F0RRTQ</td>\n",
       "      <td>A1R7E82MN0S8V3</td>\n",
       "      <td>DENNIS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-06-12</td>\n",
       "      <td>GREAT DOG TREAT</td>\n",
       "      <td>\"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418609</th>\n",
       "      <td>418610</td>\n",
       "      <td>B001F0RRU0</td>\n",
       "      <td>A1R7E82MN0S8V3</td>\n",
       "      <td>DENNIS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-06-12</td>\n",
       "      <td>GREAT DOG TREAT</td>\n",
       "      <td>\"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561246</th>\n",
       "      <td>561247</td>\n",
       "      <td>B001JU81ZG</td>\n",
       "      <td>A7FNPP1SMY97G</td>\n",
       "      <td>D. Hsu</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-11-08</td>\n",
       "      <td>Buy this if you have NO taste buds!</td>\n",
       "      <td>\"Blends smooth and creamy for a sweet tasting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48026</th>\n",
       "      <td>48027</td>\n",
       "      <td>B004SRH2B6</td>\n",
       "      <td>A13RF7W3A98FS0</td>\n",
       "      <td>utah2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>great tasting</td>\n",
       "      <td>zico probably has the best taste of the coconu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268751</th>\n",
       "      <td>268752</td>\n",
       "      <td>B004SRFYMK</td>\n",
       "      <td>A13RF7W3A98FS0</td>\n",
       "      <td>utah2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>great tasting</td>\n",
       "      <td>zico probably has the best taste of the coconu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292422</th>\n",
       "      <td>292423</td>\n",
       "      <td>B0022N49KU</td>\n",
       "      <td>A1R58LWNVV94NA</td>\n",
       "      <td>Toni T. \"AboveAstar\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>EXCELLENT Quality &amp; Taste; Very, Very Versatil...</td>\n",
       "      <td>~ I ordered Farie's Finest Coconut powder, Map...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167012</th>\n",
       "      <td>167013</td>\n",
       "      <td>B0022N49M8</td>\n",
       "      <td>A1R58LWNVV94NA</td>\n",
       "      <td>Toni T. \"AboveAstar\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>EXCELLENT Quality &amp; Taste; Very, Very Versatil...</td>\n",
       "      <td>~ I ordered Farie's Finest Coconut powder, Map...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113740</th>\n",
       "      <td>113741</td>\n",
       "      <td>B0022N26NM</td>\n",
       "      <td>A1R58LWNVV94NA</td>\n",
       "      <td>Toni T. \"AboveAstar\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>EXCELLENT Quality &amp; Taste; Very, Very Versatil...</td>\n",
       "      <td>~ I ordered Farie's Finest Coconut powder, Map...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232915 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId           ProfileName  \\\n",
       "257785  257786  B000KOWR8E  A142S4ZZF1FJ1X         Joseph E Brew   \n",
       "506745  506746  B000KOWR8Y  A142S4ZZF1FJ1X         Joseph E Brew   \n",
       "107704  107705  B001F0RRTQ  A1R7E82MN0S8V3                DENNIS   \n",
       "418609  418610  B001F0RRU0  A1R7E82MN0S8V3                DENNIS   \n",
       "561246  561247  B001JU81ZG   A7FNPP1SMY97G                D. Hsu   \n",
       "...        ...         ...             ...                   ...   \n",
       "48026    48027  B004SRH2B6  A13RF7W3A98FS0              utah2008   \n",
       "268751  268752  B004SRFYMK  A13RF7W3A98FS0              utah2008   \n",
       "292422  292423  B0022N49KU  A1R58LWNVV94NA  Toni T. \"AboveAstar\"   \n",
       "167012  167013  B0022N49M8  A1R58LWNVV94NA  Toni T. \"AboveAstar\"   \n",
       "113740  113741  B0022N26NM  A1R58LWNVV94NA  Toni T. \"AboveAstar\"   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "257785                     2                       3      4 2010-10-09   \n",
       "506745                     0                       0      4 2010-10-09   \n",
       "107704                     0                       0      5 2012-06-12   \n",
       "418609                     0                       0      5 2012-06-12   \n",
       "561246                     4                       6      1 2011-11-08   \n",
       "...                      ...                     ...    ...        ...   \n",
       "48026                      0                       0      5 2011-10-01   \n",
       "268751                     0                       0      5 2011-10-01   \n",
       "292422                     2                       2      5 2011-12-20   \n",
       "167012                     2                       2      5 2011-12-20   \n",
       "113740                     2                       2      5 2011-12-20   \n",
       "\n",
       "                                                  Summary  \\\n",
       "257785                                  Better Sweetener!   \n",
       "506745                                   4C Totally Light   \n",
       "107704                                    GREAT DOG TREAT   \n",
       "418609                                    GREAT DOG TREAT   \n",
       "561246                Buy this if you have NO taste buds!   \n",
       "...                                                   ...   \n",
       "48026                                       great tasting   \n",
       "268751                                      great tasting   \n",
       "292422  EXCELLENT Quality & Taste; Very, Very Versatil...   \n",
       "167012  EXCELLENT Quality & Taste; Very, Very Versatil...   \n",
       "113740  EXCELLENT Quality & Taste; Very, Very Versatil...   \n",
       "\n",
       "                                                     Text  \n",
       "257785  \"4C Totally Light\" is one of the very few \"sug...  \n",
       "506745  \"4C Totally Light\" is one of the very few \"sug...  \n",
       "107704  \"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...  \n",
       "418609  \"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...  \n",
       "561246  \"Blends smooth and creamy for a sweet tasting ...  \n",
       "...                                                   ...  \n",
       "48026   zico probably has the best taste of the coconu...  \n",
       "268751  zico probably has the best taste of the coconu...  \n",
       "292422  ~ I ordered Farie's Finest Coconut powder, Map...  \n",
       "167012  ~ I ordered Farie's Finest Coconut powder, Map...  \n",
       "113740  ~ I ordered Farie's Finest Coconut powder, Map...  \n",
       "\n",
       "[232915 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by text and filter to get groups where the size is greater than 1 (i.e., duplicates)\n",
    "dup_text_groups = df.groupby('Text',observed=False).filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Sort the groups by 'textCat'\n",
    "dup_text_groups_sorted = dup_text_groups.sort_values(by='Text')\n",
    "\n",
    "# Print the duplicated rows, grouped by 'textCat'\n",
    "dup_text_groups_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94d828c4-821d-4dc3-ae10-3642280af143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107704</th>\n",
       "      <td>107705</td>\n",
       "      <td>B001F0RRTQ</td>\n",
       "      <td>A1R7E82MN0S8V3</td>\n",
       "      <td>DENNIS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-06-12</td>\n",
       "      <td>GREAT DOG TREAT</td>\n",
       "      <td>\"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418609</th>\n",
       "      <td>418610</td>\n",
       "      <td>B001F0RRU0</td>\n",
       "      <td>A1R7E82MN0S8V3</td>\n",
       "      <td>DENNIS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-06-12</td>\n",
       "      <td>GREAT DOG TREAT</td>\n",
       "      <td>\"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330089</th>\n",
       "      <td>330090</td>\n",
       "      <td>B001OHX1ZY</td>\n",
       "      <td>A7FNPP1SMY97G</td>\n",
       "      <td>D. Hsu</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-11-08</td>\n",
       "      <td>Buy this if you have NO taste buds!</td>\n",
       "      <td>\"Blends smooth and creamy for a sweet tasting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561246</th>\n",
       "      <td>561247</td>\n",
       "      <td>B001JU81ZG</td>\n",
       "      <td>A7FNPP1SMY97G</td>\n",
       "      <td>D. Hsu</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-11-08</td>\n",
       "      <td>Buy this if you have NO taste buds!</td>\n",
       "      <td>\"Blends smooth and creamy for a sweet tasting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184223</th>\n",
       "      <td>184224</td>\n",
       "      <td>B006ZC3IHY</td>\n",
       "      <td>A17950SQVNAVOD</td>\n",
       "      <td>Scott</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-18</td>\n",
       "      <td>Packaging quality problem</td>\n",
       "      <td>\"Both\" of Gloria Jean's \"Hazelnut\" and \"Vanill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268751</th>\n",
       "      <td>268752</td>\n",
       "      <td>B004SRFYMK</td>\n",
       "      <td>A13RF7W3A98FS0</td>\n",
       "      <td>utah2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>great tasting</td>\n",
       "      <td>zico probably has the best taste of the coconu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358777</th>\n",
       "      <td>358778</td>\n",
       "      <td>B003CIBPN8</td>\n",
       "      <td>A13RF7W3A98FS0</td>\n",
       "      <td>utah2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>great tasting</td>\n",
       "      <td>zico probably has the best taste of the coconu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113740</th>\n",
       "      <td>113741</td>\n",
       "      <td>B0022N26NM</td>\n",
       "      <td>A1R58LWNVV94NA</td>\n",
       "      <td>Toni T. \"AboveAstar\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>EXCELLENT Quality &amp; Taste; Very, Very Versatil...</td>\n",
       "      <td>~ I ordered Farie's Finest Coconut powder, Map...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167012</th>\n",
       "      <td>167013</td>\n",
       "      <td>B0022N49M8</td>\n",
       "      <td>A1R58LWNVV94NA</td>\n",
       "      <td>Toni T. \"AboveAstar\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>EXCELLENT Quality &amp; Taste; Very, Very Versatil...</td>\n",
       "      <td>~ I ordered Farie's Finest Coconut powder, Map...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292422</th>\n",
       "      <td>292423</td>\n",
       "      <td>B0022N49KU</td>\n",
       "      <td>A1R58LWNVV94NA</td>\n",
       "      <td>Toni T. \"AboveAstar\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>EXCELLENT Quality &amp; Taste; Very, Very Versatil...</td>\n",
       "      <td>~ I ordered Farie's Finest Coconut powder, Map...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230807 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId           ProfileName  \\\n",
       "107704  107705  B001F0RRTQ  A1R7E82MN0S8V3                DENNIS   \n",
       "418609  418610  B001F0RRU0  A1R7E82MN0S8V3                DENNIS   \n",
       "330089  330090  B001OHX1ZY   A7FNPP1SMY97G                D. Hsu   \n",
       "561246  561247  B001JU81ZG   A7FNPP1SMY97G                D. Hsu   \n",
       "184223  184224  B006ZC3IHY  A17950SQVNAVOD                 Scott   \n",
       "...        ...         ...             ...                   ...   \n",
       "268751  268752  B004SRFYMK  A13RF7W3A98FS0              utah2008   \n",
       "358777  358778  B003CIBPN8  A13RF7W3A98FS0              utah2008   \n",
       "113740  113741  B0022N26NM  A1R58LWNVV94NA  Toni T. \"AboveAstar\"   \n",
       "167012  167013  B0022N49M8  A1R58LWNVV94NA  Toni T. \"AboveAstar\"   \n",
       "292422  292423  B0022N49KU  A1R58LWNVV94NA  Toni T. \"AboveAstar\"   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "107704                     0                       0      5 2012-06-12   \n",
       "418609                     0                       0      5 2012-06-12   \n",
       "330089                     4                       6      1 2011-11-08   \n",
       "561246                     4                       6      1 2011-11-08   \n",
       "184223                     0                       0      1 2011-08-18   \n",
       "...                      ...                     ...    ...        ...   \n",
       "268751                     0                       0      5 2011-10-01   \n",
       "358777                     0                       0      5 2011-10-01   \n",
       "113740                     2                       2      5 2011-12-20   \n",
       "167012                     2                       2      5 2011-12-20   \n",
       "292422                     2                       2      5 2011-12-20   \n",
       "\n",
       "                                                  Summary  \\\n",
       "107704                                    GREAT DOG TREAT   \n",
       "418609                                    GREAT DOG TREAT   \n",
       "330089                Buy this if you have NO taste buds!   \n",
       "561246                Buy this if you have NO taste buds!   \n",
       "184223                          Packaging quality problem   \n",
       "...                                                   ...   \n",
       "268751                                      great tasting   \n",
       "358777                                      great tasting   \n",
       "113740  EXCELLENT Quality & Taste; Very, Very Versatil...   \n",
       "167012  EXCELLENT Quality & Taste; Very, Very Versatil...   \n",
       "292422  EXCELLENT Quality & Taste; Very, Very Versatil...   \n",
       "\n",
       "                                                     Text  \n",
       "107704  \"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...  \n",
       "418609  \"BUFFY\" LOOKS FORWARD TO HER \"TOY\" EVERY AFTER...  \n",
       "330089  \"Blends smooth and creamy for a sweet tasting ...  \n",
       "561246  \"Blends smooth and creamy for a sweet tasting ...  \n",
       "184223  \"Both\" of Gloria Jean's \"Hazelnut\" and \"Vanill...  \n",
       "...                                                   ...  \n",
       "268751  zico probably has the best taste of the coconu...  \n",
       "358777  zico probably has the best taste of the coconu...  \n",
       "113740  ~ I ordered Farie's Finest Coconut powder, Map...  \n",
       "167012  ~ I ordered Farie's Finest Coconut powder, Map...  \n",
       "292422  ~ I ordered Farie's Finest Coconut powder, Map...  \n",
       "\n",
       "[230807 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, filter within each group for rows where 'ProductId', 'UserId', 'ProfileName' are the same\n",
    "dup_filtered = dup_text_groups.groupby(['Text','UserId','ProfileName','Summary'],observed=False).filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Sort the groups by 'textCat' and other relevant columns\n",
    "dup_filtered_sorted = dup_filtered.sort_values(by=['Text','UserId','ProfileName','Summary'])\n",
    "\n",
    "# Print the filtered and sorted duplicated rows\n",
    "dup_filtered_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d525ebd-019b-40f7-a782-0a5190c30092",
   "metadata": {},
   "source": [
    "### Step 5.1: Dropping duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2758a856-ab29-4b9e-9ca4-2e6d334156d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame shape: (395016, 10)\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates from the main DataFrame based on the specified columns\n",
    "df_deduped = df.drop_duplicates(subset=['Text','UserId', 'ProfileName', 'Summary'])\n",
    "\n",
    "# Assign it back to df if you want to overwrite the original\n",
    "df = df_deduped\n",
    "\n",
    "# Optional: check the shape after deduplication\n",
    "print(f\"Updated DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "393ad582-26d1-46ee-b2b8-2aadef94b600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, ProductId, UserId, ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary, Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by 'Text' and filter to get groups where the size is greater than 1 (i.e., duplicates)\n",
    "dup_textCat_groups = df.groupby('Text',observed=False).filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Now, filter within each group for rows where 'ProductId', 'UserId', 'ProfileName' are the same\n",
    "dup_filtered = dup_textCat_groups.groupby(['Text','UserId','ProfileName','Summary'],observed=False).filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Sort the groups by 'textCat' and other relevant columns\n",
    "dup_filtered_sorted = dup_filtered.sort_values(by=['Text','UserId','ProfileName','Summary'])\n",
    "\n",
    "# Print the filtered and sorted duplicated rows\n",
    "dup_filtered_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f8da9-8017-41da-a1e4-2a0e7cd9be79",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "### Step 1: Combining Summary and Text columns to create list of all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0fcc589-0815-4876-a334-ba9b77422515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words extracted: 34365117\n"
     ]
    }
   ],
   "source": [
    "# Combine 'summary' and 'text' columns\n",
    "combined_text = df['Summary'].fillna('') + ' ' + df['Text'].fillna('')\n",
    "\n",
    "# Extract words using regex, convert to lowercase\n",
    "all_words = []\n",
    "for entry in combined_text:\n",
    "    entry_words = re.findall(r'\\b\\w+\\b', entry.lower())\n",
    "    all_words.extend(entry_words)  # Add words to the list\n",
    "\n",
    "# Print the number of words\n",
    "print(f\"Total words extracted: {len(all_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce27b6-4aaf-4a66-8934-5617cf3410f1",
   "metadata": {},
   "source": [
    "### Step 2: Removing Nonsense words and Stop words from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5793ecb-4224-4225-9868-aae8b8147b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prarthana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'quality', 'dog', 'food', 'bought', 'several', 'vitality', 'canned', 'dog', 'food', 'products', 'found', 'good', 'quality', 'product', 'looks', 'like', 'stew', 'processed', 'meat']\n",
      "Total cleaned words: 17164436\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords list (only once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean words\n",
    "def clean_word(word):\n",
    "    # Remove non-alphabetical characters and check if it's a valid word\n",
    "    return re.match(r'^[a-zA-Z]+$', word) is not None and word not in stop_words\n",
    "\n",
    "# Assuming 'all_words' contains all words\n",
    "cleaned_words = [word for word in all_words if clean_word(word.lower())]\n",
    "\n",
    "# Print first 20 cleaned words and total count\n",
    "print(cleaned_words[:20])\n",
    "print(f\"Total cleaned words: {len(cleaned_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2537fac6-bf04-47b8-bbbc-ae435c66643e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/prarthana/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK words corpus\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "335940af-da84-4cda-bc32-4b58dc15f825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prarthana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'quality', 'dog', 'food', 'bought', 'several', 'vitality', 'canned', 'dog', 'food', 'products', 'found', 'good', 'quality', 'product', 'looks', 'like', 'stew', 'processed', 'meat']\n",
      "Total filtered words: 17587203\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords list (only once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get stop words from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming cleaned_words is already defined from previous steps\n",
    "# Filter out stop words from the cleaned words list\n",
    "filtered_words = [word for word in all_words if word.lower() not in stop_words]\n",
    "\n",
    "# Print the first 20 filtered words and total count of filtered words\n",
    "print(filtered_words[:20])\n",
    "print(f\"Total filtered words: {len(filtered_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7516d37-6924-4db6-8193-0a1b52086318",
   "metadata": {},
   "source": [
    "### Step 3:  Correcting the mispelled words\n",
    "Author's note: This takes a while( about 10 mins) to run so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2820589-2684-4b57-9462-69b7c710d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SymSpell with a max dictionary edit distance of 2\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# Load the dictionary from NLTK or create your own (e.g., words corpus)\n",
    "dictionary = set(words.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a7b296e-e9e8-4eea-8622-4ed122b8fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add words to SymSpell dictionary\n",
    "for word in dictionary:\n",
    "    sym_spell.create_dictionary_entry(word, 1)  # Frequency is set to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe2ee11e-e5b7-4b7d-afbf-dbf3f5381e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and correct words using SymSpell\n",
    "def clean_word(word):\n",
    "    # Remove non-alphabetical characters\n",
    "    if not re.match(r'^[a-zA-Z]+$', word):\n",
    "        return False\n",
    "    \n",
    "    # Correct spelling using SymSpell\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    \n",
    "    # If SymSpell found a valid suggestion, return the corrected word\n",
    "    return suggestions[0].term if suggestions else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31e79dea-971b-417b-815d-09db6711c886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'quality', 'dog', 'food', 'bought', 'several', 'vitality', 'canned', 'dog', 'food', 'products', 'found', 'good', 'quality', 'product', 'looks', 'like', 'stew', 'processed', 'meat']\n",
      "Total cleaned words: 17070744\n"
     ]
    }
   ],
   "source": [
    "# Clean the words list\n",
    "cleaned_words = [word for word in filtered_words if clean_word(word.lower())]\n",
    "\n",
    "# Print the first 20 cleaned words and total count\n",
    "print(cleaned_words[:20])\n",
    "print(f\"Total cleaned words: {len(cleaned_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a85f2653-ac9a-44cc-9001-bc4c4d6b5fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'quality', 'dog', 'food', 'bought', 'several', 'vitality', 'canned', 'dog', 'food', 'products', 'found', 'good', 'quality', 'product', 'looks', 'like', 'stew', 'processed', 'meat']\n",
      "Total cleaned words: 17070744\n"
     ]
    }
   ],
   "source": [
    "# Print the first 20 cleaned words and total count\n",
    "print(cleaned_words[:20])\n",
    "print(f\"Total cleaned words: {len(cleaned_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43edfbd7-96e1-485e-82fb-e77386ff183d",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Creating another df of unique words for future use so we have frequency of words as well as list of unique words. We wouldnt have to run sentiment analysis on the repeated words as TextBlob uses a predefined dictionary (lexicon) of words where each word has an associated polarity (how positive/negative it is) and subjectivity (how opinionated it is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae9a68-1371-4799-8de5-7bd200134d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_words2 = list(set(cleaned_words))\n",
    "\n",
    "# Print first 20 words and total count after removing duplicates\n",
    "print(cleaned_words2[:20])\n",
    "print(f\"Total cleaned words after removing duplicates: {len(cleaned_words2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b762283-474f-48a3-8f62-499cb44869d4",
   "metadata": {},
   "source": [
    "### Step 4: Classifying words into 'positive', 'negative', and 'neutral'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8537be0f-056b-43d2-a203-1c32bcae266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word Sentiment\n",
      "0      lilburn   Neutral\n",
      "1    refluxers   Neutral\n",
      "2   inheriting   Neutral\n",
      "3  rewatchable   Neutral\n",
      "4    stonhouse   Neutral\n"
     ]
    }
   ],
   "source": [
    "# Function to classify sentiment using TextBlob\n",
    "def classify_sentiment_textblob(words_list):\n",
    "    sentiment_data = []  # List to store word and sentiment pairs\n",
    "    \n",
    "    # Classify sentiment for each word in the list\n",
    "    for word in words_list:\n",
    "        polarity = TextBlob(word).sentiment.polarity\n",
    "        if polarity > 0:\n",
    "            sentiment = \"Positive\"\n",
    "        elif polarity < 0:\n",
    "            sentiment = \"Negative\"\n",
    "        else:\n",
    "            sentiment = \"Neutral\"\n",
    "        \n",
    "        sentiment_data.append([word, sentiment])\n",
    "    \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df_sentiment = pd.DataFrame(sentiment_data, columns=[\"Word\", \"Sentiment\"])\n",
    "    return df_sentiment\n",
    "\n",
    "# Example: Classify sentiment for the cleaned words\n",
    "df_sentiment = classify_sentiment_textblob(cleaned_words2)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df_sentiment.head())  # Show first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733bfbc-c372-4df8-af63-ef411eeabaec",
   "metadata": {},
   "source": [
    "### Step 5: Categorizing words, frequency and sentiment to analyse the overall sentiment being observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3df0c512-709b-439b-a57d-d5deabec2dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word   Count Sentiment\n",
      "211       br  449305   Neutral\n",
      "12      like  183433   Neutral\n",
      "0       good  175306  Positive\n",
      "92     great  165454  Positive\n",
      "10   product  135573   Neutral\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Count frequency of each word in cleaned_words\n",
    "word_count = {}\n",
    "for word in cleaned_words:\n",
    "    word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "# Create a DataFrame from word_count (Word and Count)\n",
    "word_count_df = pd.DataFrame(list(word_count.items()), columns=[\"Word\", \"Count\"])\n",
    "\n",
    "# Step 2: Merge the word count DataFrame with the sentiment DataFrame\n",
    "# We'll merge on the \"Word\" column to add sentiments to the word count DataFrame\n",
    "df_merged = pd.merge(word_count_df, df_sentiment, on=\"Word\", how=\"left\")\n",
    "\n",
    "# Step 3: Sort the merged DataFrame by count in descending order\n",
    "df_merged_sorted = df_merged.sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "# Display the resulting merged and sorted DataFrame\n",
    "print(df_merged_sorted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7880f-3587-49d3-a9b6-9646eb39a8f7",
   "metadata": {},
   "source": [
    "### Step 6: Computing the Overall Sentiment using frequency as weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be4ce2e3-fe3c-477d-b30b-e1857d43e501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sentiment Score: 0.08701425081414144\n",
      "Overall Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Map sentiment labels to numerical values\n",
    "sentiment_map = {\"Positive\": 1, \"Neutral\": 0, \"Negative\": -1}\n",
    "df_merged_sorted['SentimentValue'] = df_merged_sorted['Sentiment'].map(sentiment_map)\n",
    "\n",
    "# Compute the weighted sentiment score\n",
    "weighted_sentiment = (df_merged_sorted['SentimentValue'] * df_merged_sorted['Count']).sum() / df_merged_sorted['Count'].sum()\n",
    "\n",
    "# Determine overall sentiment label\n",
    "if weighted_sentiment > 0:\n",
    "    overall_sentiment = \"Positive\"\n",
    "elif weighted_sentiment < 0:\n",
    "    overall_sentiment = \"Negative\"\n",
    "else:\n",
    "    overall_sentiment = \"Neutral\"\n",
    "\n",
    "# Output the results\n",
    "print(f\"Weighted Sentiment Score: {weighted_sentiment}\")\n",
    "print(f\"Overall Sentiment: {overall_sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
